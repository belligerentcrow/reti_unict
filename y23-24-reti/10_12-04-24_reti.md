# 10 Lezione -- Reti di Calcolatori

---

<!-- TOC -->
- [Recap comprensivo -- congestione e soluzioni](#recap-comprensivo----congestione-e-soluzioni)
    - [note sul controllo congestione](#note-sul-controllo-congestione)
- [Dimostrazione della TCP fairness --  Spartizione di connessioni. Triangolo](#dimostrazione-della-tcp-fairness-----spartizione-di-connessioni-triangolo)
    - [Pero' ...](#pero-)
- [HTTP3.0](#http30)
    - [Caratteristiche](#caratteristiche)
<!-- /TOC -->

---

## Recap comprensivo -- congestione e soluzioni

-> Rallentamento != Congestione: Nel rallentamento i pacchetti alla fine arrivano.  
* TCP : Deve garantire la consegna. Deve avere il canale di ritorno con gli ACK. Se i messaggi non arrivano c'e' la ritrasmissione. Se il canale di ritorno e' intasato il mittente rispedisce sempre gli stessi pacchetti che magari sono gia' arrivati a destinazione e il destinatario continua a ricevere pacchetti inutili. Ma manda di nuovo gli ACK che vanno a peggiorare la situazione.  

* Il sistema della congestione si **autoalimenta**. Piu' si va avanti peggio e'. Il punto non e' che il canale "e' pieno", ma e' che tiene roba inutile in un circolo vizioso. L'unica soluzione che puo' fare TCP e' il fermare la comunicazione in maniera da dare tempo ai router di svuotare i propri buffers. Come fa il Sender TCP a capire che siamo arrivati alla congestione? La situazione ideale e' avere pieno il link piu' scarso (?).   
Non posso pensare di avere tutti i link pieni e che tutto funzioni. Il punto e' che il mittente potrebbe vedere solo il canale in uscita.  
La tecnica TCP e' mandare pacchetti sempre piu' velocemente fino a saturare questo link.  
Il sender per capire che non puo' andare avanti guarda gli ack di ritorno. Idea: Arrivato l'ack, bene. Ma **non usa stop and wait**.   
Ad un certo punto inizio a rallentare in base ad una soglia. Slow start + incremento additivo quando arriviamo sopra la soglia: iniziamo ad avere perdite. La dimensione della finestra di spedizione si alza; la soglia si adatta   

* Quale canale? Man mano che inizio ad andare piu' veloce magari ho saturato il collo di bottiglia. Il traffico aggiuntivo finisce nei buffer dei router di transito. Li mettiamo li' per evitare che picchi di traffico siano buttati, ma ad un certo punto le code si riempiono e il traffico viene buttato (al destinatario manca qualcosa.)   
  
1. Perdita di un singolo pacchetto --> Compensiamo con il fast retrasmit
2. Perdita di piu' pacchetti che determinano lo scadere del timer. 
  
Queste sono quello che vede il sender, il campanello di allarme della confusione/congestione di rete.  
  
Nota: **TCP non puo' distinguere come i pacchetti sono stati persi** (perdita di frame? congestione di rete? Buffer saturati?)  ma tra Tahoe e Reno c'e' la differenza di come trattare i problemi

Tahoe parte sempre da 0 mentre Reno riparte da meta' di dove c'e' stato il problema SE ci sono stati troppi ACK, altrimenti se si tratta di Timeout comunuque riparte da 0.  
  
Altri problemi: quando il canale ha una grossa larghezza di banda, arrivare al limite e' molto molto difficile. Un protocollo datato quindi non sfrutta un buon canale nuovo come potrebbe.  
  
* Variante TCP per i data center: 

* Nota: HTTP3 prevede l'uso di UDP.  
  
* Differenza tra chiamate Read bloccanti e non bloccanti.  

* **IDEA** -> Guardare i pacchetti che ricevo --> Il ritmo con il quale tornano.  
Il pacchetto arriva in ritardo, gli ack partono in orario e arrivano distanziati. Se il ritardo e' molto potrebbero essersi riempiti i buffers.  

* Schema: Piu' di 100 varianti di TCP.  Reattive(loss-based), proattive (prevengo il problema) (Delay based, degli ack), reactive (con stima della banda, del canale che fa da collo di bottiglia)  
  
L'ideale sarebbe sapere cosa sta succedendo: allora dato che c'erano dei flag liberi nella intestazione di TCP, si fa in modo che i Router settino i flag se sta avvenendo connessione: considerare che in questo caso anche i router devono supportare questo protocollo (e violano cosi')  
  
### note sul controllo congestione
TIMERS --> Sono stati studiati per migliorare la congestione e fanno parte del controllo della congestione. Indirettamente

Il controllo del flusso? Direttamente no ma indirettamente si', perche' se mando pacchetti inutili allora aumento il traffico di rete, quindi aumento il rischio di riempire i buffer, e fare in modo che funzioni bene fa parte del controllo complessivo.    
  
## Dimostrazione della TCP fairness --  Spartizione di connessioni. Triangolo

Soluzione banale per il fatto che due connessioni si dividano la banda: dare ticket. Ma i router non devono intervenire al livello di trasporto. I due sender TCP non sanno che esistono MA.  
  
Dimostriamo che le due connessioni riescono a dividersi la connessione in due.  
  
Due assi -> Throughput di connessione 1, Throughput di connessione 2. Non esistono gli altri quadranti.  
  
Dal punto P(x,y) --> imponiamo che la somma di x e y sia uguale a R 
  
```R = x+y --> quindi y = -x + R```  

Quindi ```m = -1, n = R```. Quando m = -1, la retta e' parallela alla seconda bisettrice. Il fatto che imponiamo x+y=R, imponiamo che sia una retta.  
Quindi i punti intermedi tra R e R sono la percentuale di bandwith che il canale sta usando. R su y: tutto usato da connessione2. R su x, tutta usata dalla connessione1.  
  
Bisettrice = condizione di fairness. Intersezione tra la bisettrice (fairness) e la capacita' massima del canale: troviamo Q(x,y).  
  
* Nota: questa dimostrazione funziona anche per piu' di 2 connessioni. Una connessione per asse.  
  
Oltre la maxchannelcapacity posso andarci perche' nel frattempo inizio ad andare nel buffer. Posos andare piu' veloce di R come somma delle connessioni. Ma non ci si puo' restare a lungo (perche' i buffers si riempiono)  
  
Situazione di congestion avoidance: additive increase. Tutte e due aumentano di un segmento.  
  
Equazione della condizione della rete in questo momento:   ```y = x + n```
 
Aumentiamo:  ```y+1 = m(x+1) + n``` da cui ``` 1=m
Quindi quella retta e' una parallela alla bisettrice. 
  
Ad un certo punto pero' c'e' una perdita. Immaginiamo che perdano entrambe pacchetti. P'(x/2, y/2).  
Retta generica che passa per i due punti P(x,y) e P' -> y = mx.  Ha la proprieta' di passare per l'origine.  
Quindi il triangolo che si forma quando dimezzo --> Incremento parallelo, decremento per una retta che passa per l'origine.  
  
Metafora: una sfera che scende e risale in un parco da skate. Il punto di equilibrio e' in fondo (e in questo caso tra le due connessioni e' la retta di equal bandwidth share che e' la bisettrice del quadrante.)   

### Pero' ...

1. Stiamo ipotizzando che i pacchetti perdono pacchetti assieme.  
2. Questo schema funziona se gli MSS di entrambe le connessioni sono uguali. Se una delle due connessioni ha un MSS maggiore si freghera' piu' rete.  
  
Piu' o meno si vanno ad equilibrare quando le perdite sono simili/uguali  

* Perche' funziona?  --> Ipotesi antropocentrica forte  ...  
Se cresco piu' velocemente di quanto decremento il sistema non funziona e si squilibra tutto da un altro lato.  

## HTTP3.0 

Vieta di aprire connessioni HTTP multiple verso lo stesso server.  
Affronto il problema facendo un polling nel trasferimento degli oggetti.  
Se si ha una pagina con tanti oggetti puo' convenire aprire una connessione HTTP per ogni oggetto --> interattivita', etc. Se ne faccio una sola, prima prendo uno, poi il secondo, terzo, etc.  Ma se il primo e' pesante, la pagina diventa vuota.  
Se ne faccio di piu' le connessioni sono serializzate. --> Cosi' all'utente arrivano degli oggetti.  

### Caratteristiche

Apro una sola connessione HTTP e faccio un polling e scheduling e **applico il roundrobin** e do' un quanto di tempo per ogni oggetto, e' ancora meglio. Questo e' quello che fa HTTP3.  